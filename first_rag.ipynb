{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Environment & Setup\n",
    "Building a production RAG system, started in a notebook to **prototype components** before moving them to Python modules.\n",
    "\n",
    "### Step 1: Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env file\n",
      "‚úÖ API Key found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Adjust 'RAG systems using LlamaIndex' if your folder name is different\n",
    "env_path = Path(\"..\") / \"RAG systems using LlamaIndex\" / \".env\"\n",
    "\n",
    "# 2. Load it\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(\"‚úÖ Loaded .env file\")\n",
    "else:\n",
    "    print(f\"‚ùå Could not find .env at: {env_path.resolve()}\")\n",
    "\n",
    "# 3. Verify\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"‚úÖ API Key found\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API Key not found. Check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: The Ingestion Logic\n",
    "**Design Decision**: Clone **Everything**, Filter **Later**.\n",
    "*   **Cloning**: Use `git clone`. It's faster and simpler to just pull the whole repo than to try and \"sparse checkout\" file-by-file.\n",
    "*   **Filtering**: We will apply a **Blacklist** (exclude `.png`, `.pth`, `.git`) during the document loading phase. This keeps the local disk faithful to reality but protects the embedding model from garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è Repo already exists at: github_rag_engine/data/repos/Diff-Retinex-Plus\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# CONFIG\n",
    "REPO_URL = \"https://github.com/XunpengYi/Diff-Retinex-Plus\"\n",
    "STORAGE_PATH = Path(\"./github_rag_engine/data/repos\")\n",
    "REPO_NAME = REPO_URL.split(\"/\")[-1]\n",
    "LOCAL_REPO_PATH = STORAGE_PATH / REPO_NAME\n",
    "\n",
    "def clone_repo(url, path):\n",
    "    if path.exists():\n",
    "        print(f\"‚ÑπÔ∏è Repo already exists at: {path}\")\n",
    "        # In production, we would git pull here\n",
    "        return\n",
    "    \n",
    "    print(f\"‚è≥ Cloning {url}...\")\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", url, str(path)], check=True)\n",
    "        print(\"‚úÖ Clone successful\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Clone failed: {e}\")\n",
    "\n",
    "# 1. Run the clone\n",
    "clone_repo(REPO_URL, LOCAL_REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Filtering & Processing\n",
    "Now we iterate through the files. \n",
    "\n",
    "**The Rule**: skip if extension is in `IGNORED_EXTENSIONS` OR if it is a hidden file/directory (starts with `.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORED_EXTENSIONS = {\n",
    "    # Images\n",
    "    \".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".ico\",\n",
    "    # Compiled / Binary\n",
    "    \".pyc\", \".o\", \".exe\", \".dll\", \".so\", \".dylib\",\n",
    "    # Models / Data\n",
    "    \".pth\", \".pt\", \".onnx\", \".pkl\", \".bin\", \".data\",\n",
    "    # Git\n",
    "    \".git\",\n",
    "    # Lock files (Optional - debating this)\n",
    "    \".lock\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_source_files(directory):\n",
    "    source_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # 1. Remove hidden dirs like .git in-place to avoid traversing them\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "        \n",
    "        for file in files:\n",
    "            # 2. Skip hidden files\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "                \n",
    "            # 3. Check extension blacklist\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            if ext in IGNORED_EXTENSIONS:\n",
    "                continue\n",
    "            \n",
    "            full_path = Path(root) / file\n",
    "            source_files.append(full_path)\n",
    "            \n",
    "    return source_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 valid source files\n",
      "Sample: ['test.py', 'README.md', 'test_UHD.py', 'train.py', 'environment.yaml']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the filter\n",
    "files = get_source_files(LOCAL_REPO_PATH)\n",
    "print(f\"Found {len(files)} valid source files\")\n",
    "print(\"Sample:\", [f.name for f in files[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The \"Repo Map\" (The \"What was here\" context)\n",
    "You made a great point: if we skip `.pth` files, the LLM won't know they exist. \n",
    "\n",
    "**The Solution**: We create a single special document called the **Repo Map**. \n",
    "This is a text file containing the tree structure of the *entire* repo (including the files we ignored). We index this first. This gives the application \"Spatial Awareness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repo_map(root_dir):\n",
    "    lines = []\n",
    "    root_path = Path(root_dir)\n",
    "\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if \".git\" in dirs:\n",
    "            dirs.remove(\".git\")\n",
    "\n",
    "        for f in files:\n",
    "            rel_path = Path(root, f).relative_to(root_path)\n",
    "            ext = rel_path.suffix.lower()\n",
    "\n",
    "            marker = \"\"\n",
    "            if f.startswith(\".\") or ext in IGNORED_EXTENSIONS:\n",
    "                marker = \" [IGN]\"\n",
    "\n",
    "            lines.append(f\"{rel_path.as_posix()}{marker}\")\n",
    "\n",
    "    return \"\\n\".join(sorted(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REPO MAP PREVIEW ---\n",
      "README.md\n",
      "archs/DRNet_Retinex_arch.py\n",
      "archs/MoE_arch.py\n",
      "archs/__init__.py\n",
      "archs/ddpm_arch.py\n",
      "archs/decom_arch.py\n",
      "archs/sr3unet_arch.py\n",
      "asserts/framework.png [IGN]\n",
      "data/__init__.py\n",
      "data/lol_dataset.py\n",
      "dataset/put_the_dataset_here.md\n",
      "decom_tools/decom_arch.py\n",
      "decom_tools/loss_decom_TDN.py\n",
      "decom_tools/my_dataset.py\n",
      "decom_tools/test_decom_checkpoint.py\n",
      "decom_tools/train_decom.py\n",
      "decom_tools/transforms.py\n",
      "decom_tools/utils.py\n",
      "decom_weights/put_the_decom_weights_here.md\n",
      "environment.yaml\n",
      "lolv1_offical_results/1.png [IGN]\n",
      "lolv1_offical_results/111.png [IGN]\n",
      "lolv1_offical_results/146.png [IGN]\n",
      "lolv1_offical_results/179.png [IGN]\n",
      "lolv1_offical_results/22.png [IGN]\n",
      "lolv1_offical_results/23.png [IGN]\n",
      "lolv1_offical_results/493.png [IGN]\n",
      "lolv1_offical_results/547.png [IGN]\n",
      "lolv1_offical_results/55.png [IGN]\n",
      "lolv1_offical_results/665.png [IGN]\n",
      "lolv1_offical_results/669.png [IGN]\n",
      "lolv1_offical_results/748.png [IGN]\n",
      "lolv1_offical_results/778.png [IGN]\n",
      "lolv1_offical_results/780.png [IGN]\n",
      "lolv1_offical_results/79.png [IGN]\n",
      "losses/__init__.py\n",
      "losses/example_loss.py\n",
      "models/DRNet_model.py\n",
      "models/__init__.py\n",
      "options/test_diff-retinex_LLRW.yml\n",
      "options/test_diff_retinex_plus_A-LLIE.yml\n",
      "options/test_diff_retinex_plus_UHD.yml\n",
      "options/test_diff_retinex_plus_lolv1.yml\n",
      "options/test_diff_retinex_plus_lolv2_real.yml\n",
      "options/test_diff_retinex_plus_lolv2_syn.yml\n",
      "options/test_diff_retinex_plus_lsrw_huawei.yml\n",
      "options/test_diff_retinex_plus_lsrw_nikon.yml\n",
      "options/train_diff_retinex_plus_A_LLIE.yml\n",
      "options/train_diff_retinex_plus_lolv1.yml\n",
      "options/train_diff_retinex_plus_lolv2_real.yml\n",
      "options/train_diff_retinex_plus_lolv2_syn.yml\n",
      "options/train_diff_retinex_plus_lsrw.yml\n",
      "pretrained_weights/put_the_pretrained_weights_here.md\n",
      "scripts/options.py\n",
      "scripts/prepare_example_data.py\n",
      "scripts/utils.py\n",
      "test.py\n",
      "test_UHD.py\n",
      "train.py\n",
      "train_pipline.py\n"
     ]
    }
   ],
   "source": [
    "# Generate and Print\n",
    "repo_map_str = generate_repo_map(LOCAL_REPO_PATH)\n",
    "print(\"--- REPO MAP PREVIEW ---\")\n",
    "print(repo_map_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Loading with Metadata\n",
    "Now we load the *content* of the valid files. \n",
    "\n",
    "**Crucial**: We inject `file_metadata`. Every chunk must know its own filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Repo Map Document\n",
    "map_doc = Document(\n",
    "    text=repo_map_str,\n",
    "    metadata={\"file_path\": \"REPO_STRUCTURE.txt\", \"type\": \"structure\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# 2. To extract metadata from files\n",
    "def file_metadata_extractor(file_path):\n",
    "    # Get modification time\n",
    "    #stat = os.stat(file_path)\n",
    "    #timestamp = stat.st_mtime\n",
    "    # Format as readable string\n",
    "    #last_mod = datetime.datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return {\n",
    "        \"file_path\": file_path,\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"extension\": os.path.splitext(file_path)[1],\n",
    "        \"repo_path\": os.path.dirname(file_path),\n",
    "        #\"last_modified\": last_mod\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 44 code documents + 1 Repo Map\n"
     ]
    }
   ],
   "source": [
    "if not files:\n",
    "    print(\"‚ö†Ô∏è No files found to index!\")\n",
    "    documents = []\n",
    "else:\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_files=[str(f) for f in files],\n",
    "        file_metadata=file_metadata_extractor\n",
    "    )\n",
    "    code_documents = reader.load_data()\n",
    "    all_documents = [map_doc] + code_documents\n",
    "    print(f\"‚úÖ Loaded {len(code_documents)} code documents + 1 Repo Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus/test.py', 'file_name': 'test.py', 'extension': '.py', 'repo_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus'}\n"
     ]
    }
   ],
   "source": [
    "print(code_documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Settings (OpenAI)\n",
    "We use **OpenAI** for both embedding and LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_llm_settings():\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    print(\"‚úÖ Configured: text-embedding-3-small & GPT-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configured: text-embedding-3-small & GPT-4o-mini\n"
     ]
    }
   ],
   "source": [
    "configure_llm_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Adaptive Chunking (File-Type Aware & Tiktoken)\n",
    "You are absolutely right. `.py` is handled by `CodeSplitter`, but `.md` needs `MarkdownNodeParser`, and `.yaml` needs standard text splitting.\n",
    "\n",
    "**Refinement**: We use **tiktoken** to ensure our chunks are measured in *tokens*, not characters. This is critical for maximizing the LLM's context window usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.node_parser import CodeSplitter, MarkdownNodeParser, SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_adaptive(docs):\n",
    "    alln = []\n",
    "    tok = tiktoken.encoding_for_model(\"gpt-4o-mini\").encode\n",
    "\n",
    "    ps = CodeSplitter(\n",
    "        language=\"python\",\n",
    "        chunk_lines=150,#125\n",
    "        chunk_lines_overlap=30,\n",
    "        max_chars=3000#2500\n",
    "    )\n",
    "\n",
    "    ms = MarkdownNodeParser(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    ts = SentenceSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        tokenizer=tok\n",
    "    )\n",
    "\n",
    "    for doc in docs:\n",
    "        fn = doc.metadata.get(\"file_name\", \"\")\n",
    "        ext = doc.metadata.get(\"extension\", \"\").lower()\n",
    "\n",
    "        if ext == \".py\":\n",
    "            nds = ps.get_nodes_from_documents([doc])\n",
    "        elif ext == \".md\":\n",
    "            nds = ms.get_nodes_from_documents([doc])\n",
    "        else:\n",
    "            nds = ts.get_nodes_from_documents([doc])\n",
    "\n",
    "        for i, n in enumerate(nds):\n",
    "            n.metadata[\"chunk_id\"] = i\n",
    "            n.metadata[\"file_ext\"] = ext\n",
    "            n.metadata[\"file_name\"] = fn\n",
    "\n",
    "            pre = nds[i - 1].text if i > 0 else \"\"\n",
    "            post = nds[i + 1].text if i < len(nds) - 1 else \"\"\n",
    "\n",
    "            n.metadata[\"pre_context\"] = pre\n",
    "            n.metadata[\"post_context\"] = post\n",
    "\n",
    "        alln.extend(nds)\n",
    "\n",
    "    return alln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e941c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_nodes(nodes, sample_exts=(\".py\", \".md\", \".yaml\", \".yml\")):\n",
    "    seen = set()\n",
    "    for n in nodes:\n",
    "        ext = n.metadata.get(\"file_ext\", \"\")\n",
    "        if ext in sample_exts and ext not in seen:\n",
    "            seen.add(ext)\n",
    "            print(\"\\n--- Sample Chunk for\", ext, n.metadata.get(\"file_name\", \"\"))\n",
    "            print(\"[Node Type: {}]\".format(n.class_name()))\n",
    "            print(\"chunk_id:\", n.metadata.get(\"chunk_id\"))\n",
    "            txt = n.text[:400].replace(\"\\n\", \"\\\\n\")\n",
    "            print(txt + \"...\")\n",
    "            pre = n.metadata.get(\"pre_context\", \"\")\n",
    "            post = n.metadata.get(\"post_context\", \"\")\n",
    "            print(\"\\npre_context (first 200 chars):\")\n",
    "            print(pre[:200].replace(\"\\n\", \"\\\\n\"))\n",
    "            print(\"\\npost_context (first 200 chars):\")\n",
    "            print(post[:200].replace(\"\\n\", \"\\\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216947a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated nodes: 170\n",
      "\n",
      "--- Sample Chunk for .py test.py\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "import logging\\nimport torch\\nfrom os import path as osp\\nimport archs\\nimport data\\nimport losses\\nimport models\\n\\nfrom basicsr.data import build_dataloader, build_dataset\\nfrom basicsr.models import build_model\\nfrom basicsr.utils import get_env_info, get_root_logger, get_time_str, make_exp_dirs\\nfrom basicsr.utils.options import dict2str, parse_options\\n\\n\\ndef test_pipeline(root_path):\\n    # parse options,...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "\n",
      "\n",
      "--- Sample Chunk for .md README.md\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "# [TPAMI 2025] Diff-Retinex++: Retinex-Driven Reinforced Diffusion Model for Low-Light Image Enhancement...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "### [Paper](https://ieeexplore.ieee.org/abstract/document/10974676) | [Code](https://github.com/XunpengYi/Diff-Retinex-Plus) \\n\\n**Diff-Retinex++: Retinex-Driven Reinforced Diffusion Model for Low-Light\n",
      "\n",
      "--- Sample Chunk for .yaml environment.yaml\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "name: diff_retinex_plus_env\\nchannels:\\n  - pytorch\\n  - conda-forge\\n  - defaults\\ndependencies:\\n  - _libgcc_mutex=0.1=conda_forge\\n  - _openmp_mutex=4.5=2_gnu\\n  - aom=3.5.0=h27087fc_0\\n  - blas=1.0=mkl\\n  - bzip2=1.0.8=h4bc722e_7\\n  - ca-certificates=2024.8.30=hbcca054_0\\n  - cairo=1.16.0=h35add3b_1015\\n  - cudatoolkit=11.3.1=hb98b00a_13\\n  - dav1d=1.2.1=hd590300_0\\n  - ffmpeg=6.0.0=gpl_hdbbbd96_103\\n  - font...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "7.9=hb077bed_0\\n  - graphite2=1.3.13=h59595ed_1003\\n  - harfbuzz=7.3.0=hdb3a94d_0\\n  - icu=72.1=hcb278e6_0\\n  - intel-openmp=2022.0.1=h06a4308_3633\\n  - jpeg=9e=h0b41bf4_3\\n  - lame=3.100=h166bdaf_1003\\n  - \n",
      "\n",
      "--- Sample Chunk for .yml train_diff_retinex_plus_lolv2_real.yml\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "# general settings\\nname: diff-retinex_plus_lolv2_real_train\\nmodel_type: DRNetModel\\nnum_gpu: 2  # set num_gpu: 0 for cpu mode\\nmanual_seed: 1234\\n\\n# dataset and data loader settings\\ndatasets:\\n  train:\\n    name: train\\n    type: LoLDataset\\n    gt_root: \"./dataset/LOL-v2/Real_captured/Train/Normal\"\\n    input_root: \"./dataset/LOL-v2/Real_captured/Train/Low\"\\n    input_mode: crop\\n    crop_size: 160\\n\\n    co...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "# data loader\\n    use_shuffle: true\\n    num_worker_per_gpu: 16\\n    batch_size_per_gpu: 6\\n    dataset_enlarge_ratio: 1\\n    prefetch_mode: ~\\n\\n  val:\\n    name: LOL\\n    type: LoLDataset\\n    gt_root: \"./da\n",
      "nodes with empty file_ext: 0\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "nodes = get_nodes_adaptive(code_documents)\n",
    "print(\"generated nodes:\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_nodes(nodes)\n",
    "empty_exts = [n for n in nodes if not n.metadata.get(\"file_ext\")]\n",
    "print(\"nodes with empty file_ext:\", len(empty_exts))\n",
    "if empty_exts:\n",
    "    for n in empty_exts[:5]:\n",
    "        print(\"file_name:\", n.metadata.get(\"file_name\"), \"chunk_id:\", n.metadata.get(\"chunk_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b8abf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus/scripts/options.py',\n",
       " 'file_name': 'options.py',\n",
       " 'extension': '.py',\n",
       " 'repo_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus/scripts',\n",
       " 'chunk_id': 3,\n",
       " 'file_ext': '.py',\n",
       " 'pre_context': 'parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'-opt\\', type=str, default=\"options/train_diff_retinex_plus_lolv1.yml\", help=\\'Path to option YAML file.\\')\\n    parser.add_argument(\\'--launcher\\', choices=[\\'none\\', \\'pytorch\\', \\'slurm\\'], default=\\'none\\', help=\\'job launcher\\')\\n    parser.add_argument(\\'--auto_resume\\', action=\\'store_true\\')\\n    parser.add_argument(\\'--debug\\', action=\\'store_true\\')\\n    parser.add_argument(\\'--local_rank\\', type=int, default=0)\\n    parser.add_argument(\\n        \\'--force_yml\\', nargs=\\'+\\', default=None, help=\\'Force to update yml files. Examples: train:ema_decay=0.999\\')\\n    args = parser.parse_args()\\n\\n    # parse yml to dict\\n    with open(args.opt, mode=\\'r\\') as f:\\n        opt = yaml.load(f, Loader=ordered_yaml()[0])\\n\\n    # distributed settings\\n    if args.launcher == \\'none\\':\\n        opt[\\'dist\\'] = False\\n        print(\\'Disable distributed.\\', flush=True)\\n    else:\\n        opt[\\'dist\\'] = True\\n        if args.launcher == \\'slurm\\' and \\'dist_params\\' in opt:\\n            init_dist(args.launcher, **opt[\\'dist_params\\'])\\n        else:\\n            init_dist(args.launcher)\\n    opt[\\'rank\\'], opt[\\'world_size\\'] = get_dist_info()\\n\\n    # random seed\\n    seed = opt.get(\\'manual_seed\\')\\n    if seed is None:\\n        seed = random.randint(1, 10000)\\n        opt[\\'manual_seed\\'] = seed\\n    set_random_seed(seed + opt[\\'rank\\'])\\n\\n    # force to update yml options\\n    if args.force_yml is not None:\\n        for entry in args.force_yml:\\n            # now do not support creating new keys\\n            keys, value = entry.split(\\'=\\')\\n            keys, value = keys.strip(), value.strip()\\n            value = _postprocess_yml_value(value)\\n            eval_str = \\'opt\\'\\n            for key in keys.split(\\':\\'):\\n                eval_str += f\\'[\"{key}\"]\\'\\n            eval_str += \\'=value\\'\\n            # using exec function\\n            exec(eval_str)\\n\\n    opt[\\'auto_resume\\'] = args.auto_resume\\n    opt[\\'is_train\\'] = is_train\\n\\n    # debug setting\\n    if args.debug and not opt[\\'name\\'].startswith(\\'debug\\'):\\n        opt[\\'name\\'] = \\'debug_\\' + opt[\\'name\\']\\n\\n    if opt[\\'num_gpu\\'] == \\'auto\\':\\n        opt[\\'num_gpu\\'] = torch.cuda.device_count()\\n\\n    # datasets\\n    for phase, dataset in opt[\\'datasets\\'].items():\\n        phase = phase.split(\\'_\\')[0]\\n        dataset[\\'phase\\'] = phase\\n        if \\'scale\\' in opt:\\n            dataset[\\'scale\\'] = opt[\\'scale\\']\\n        if dataset.get(\\'dataroot_gt\\') is not None:\\n            dataset[\\'dataroot_gt\\'] = osp.expanduser(dataset[\\'dataroot_gt\\'])\\n        if dataset.get(\\'dataroot_lq\\') is not None:\\n            dataset[\\'dataroot_lq\\'] = osp.expanduser(dataset[\\'dataroot_lq\\'])\\n\\n    # paths\\n    for key, val in opt[\\'path\\'].items():\\n        if (val is not None) and (\\'resume_state\\' in key or \\'pretrain_network\\' in key):\\n            opt[\\'path\\'][key] = osp.expanduser(val)',\n",
       " 'post_context': \"@master_only\\ndef copy_opt_file(opt_file, experiments_root):\\n    import sys\\n    import time\\n    from shutil import copyfile\\n    cmd = ' '.join(sys.argv)\\n    filename = osp.join(experiments_root, osp.basename(opt_file))\\n    copyfile(opt_file, filename)\\n\\n    with open(filename, 'r+') as f:\\n        lines = f.readlines()\\n        lines.insert(0, f'# GENERATE TIME: {time.asctime()}\\\\n# CMD:\\\\n# {cmd}\\\\n\\\\n')\\n        f.seek(0)\\n        f.writelines(lines)\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[99].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331ac5d",
   "metadata": {},
   "source": [
    "# Step 8: Embeddings with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3261fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.storage.kvstore import SimpleKVStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6778209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup the Cache Storage\n",
    "CACHE_FILE = \"./github_rag_engine/cache/ingestion_cache.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65ac3f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading ingestion cache from ./github_rag_engine/cache/ingestion_cache.json...\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(f\"üìñ Loading ingestion cache from {CACHE_FILE}...\")\n",
    "    kv_store = SimpleKVStore.from_persist_path(CACHE_FILE)\n",
    "else:\n",
    "    print(\"‚ú® Creating new ingestion cache...\")\n",
    "    kv_store = SimpleKVStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "282593e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the Pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        Settings.embed_model  # The embedding model is configured earlier\n",
    "    ],\n",
    "    cache=IngestionCache(cache=kv_store)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d601e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Generating embeddings for 170 nodes (with caching)...\n",
      "‚úÖ Embeddings generated and cache saved to ./github_rag_engine/cache/ingestion_cache.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Generating embeddings for {len(nodes)} nodes (with caching)...\")\n",
    "nodes_with_embeddings = pipeline.run(nodes=nodes)\n",
    "kv_store.persist(CACHE_FILE)\n",
    "print(f\"‚úÖ Embeddings generated and cache saved to {CACHE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeb47a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Embedding (First 5 dims): [-0.014153656549751759, -0.018069013953208923, 0.06104068085551262, -0.055009569972753525, -0.01625724881887436]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample Embedding (First 5 dims): {nodes_with_embeddings[0].embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6ed7007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes_with_embeddings[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "244ad54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a205602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_counts(nodes, model_name=\"gpt-4o-mini\"):\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    total_tokens = 0\n",
    "    counts = []\n",
    "    print(f\"üìä Analyzing {len(nodes)} nodes with tiktoken ({model_name})...\")\n",
    "    for i, node in enumerate(nodes):\n",
    "        # method .get_content() ensures we get the text formatted as the LLM sees it\n",
    "        tokens = encoding.encode(node.get_content())\n",
    "        count = len(tokens)\n",
    "        # 2. Inject into metadata (Useful for debugging!)\n",
    "        #node.metadata[\"token_count\"] = count\n",
    "        \n",
    "        counts.append(count)\n",
    "        total_tokens += count\n",
    "        # Print a few samples\n",
    "        if i%20: \n",
    "            print(f\"   [Node {i}] {node.metadata.get('file_name')}: {count} tokens\")\n",
    "\n",
    "    # 3. Summary Stats\n",
    "    avg_tokens = sum(counts) / len(counts) if counts else 0\n",
    "    max_tokens = max(counts) if counts else 0\n",
    "    \n",
    "    print(f\"\\n--- Token Stats ---\")\n",
    "    print(f\"‚úÖ Total Tokens: {total_tokens:,}\")\n",
    "    print(f\"   Average:      {avg_tokens:.1f}\")\n",
    "    print(f\"   Max:          {max_tokens}\")\n",
    "    print(f\"   Min:          {min(counts) if counts else 0}\")\n",
    "    \n",
    "    # Cost Estimate (gpt-4o-mini input is ~$0.15 per 1M tokens)\n",
    "    cost = (total_tokens / 1_000_000) * 0.15\n",
    "    print(f\"   Est. Cost:    ${cost:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e233ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing 170 nodes with tiktoken (gpt-4o-mini)...\n",
      "   [Node 1] README.md: 29 tokens\n",
      "   [Node 2] README.md: 98 tokens\n",
      "   [Node 3] README.md: 57 tokens\n",
      "   [Node 4] README.md: 204 tokens\n",
      "   [Node 5] README.md: 480 tokens\n",
      "   [Node 6] README.md: 294 tokens\n",
      "   [Node 7] README.md: 436 tokens\n",
      "   [Node 8] README.md: 185 tokens\n",
      "   [Node 9] test_UHD.py: 454 tokens\n",
      "   [Node 10] train.py: 80 tokens\n",
      "   [Node 11] environment.yaml: 466 tokens\n",
      "   [Node 12] environment.yaml: 458 tokens\n",
      "   [Node 13] environment.yaml: 471 tokens\n",
      "   [Node 14] environment.yaml: 469 tokens\n",
      "   [Node 15] environment.yaml: 464 tokens\n",
      "   [Node 16] environment.yaml: 468 tokens\n",
      "   [Node 17] environment.yaml: 158 tokens\n",
      "   [Node 18] train_pipline.py: 320 tokens\n",
      "   [Node 19] train_pipline.py: 451 tokens\n",
      "   [Node 21] train_pipline.py: 19 tokens\n",
      "   [Node 22] train_pipline.py: 661 tokens\n",
      "   [Node 23] train_pipline.py: 517 tokens\n",
      "   [Node 24] train_pipline.py: 36 tokens\n",
      "   [Node 25] transforms.py: 758 tokens\n",
      "   [Node 26] transforms.py: 60 tokens\n",
      "   [Node 27] loss_decom_TDN.py: 630 tokens\n",
      "   [Node 28] loss_decom_TDN.py: 713 tokens\n",
      "   [Node 29] decom_arch.py: 697 tokens\n",
      "   [Node 30] decom_arch.py: 304 tokens\n",
      "   [Node 31] decom_arch.py: 912 tokens\n",
      "   [Node 32] decom_arch.py: 266 tokens\n",
      "   [Node 33] decom_arch.py: 6 tokens\n",
      "   [Node 34] decom_arch.py: 776 tokens\n",
      "   [Node 35] decom_arch.py: 277 tokens\n",
      "   [Node 36] my_dataset.py: 319 tokens\n",
      "   [Node 37] train_decom.py: 80 tokens\n",
      "   [Node 38] train_decom.py: 4 tokens\n",
      "   [Node 39] train_decom.py: 570 tokens\n",
      "   [Node 41] train_decom.py: 208 tokens\n",
      "   [Node 42] utils.py: 695 tokens\n",
      "   [Node 43] utils.py: 647 tokens\n",
      "   [Node 44] utils.py: 472 tokens\n",
      "   [Node 45] utils.py: 766 tokens\n",
      "   [Node 46] utils.py: 240 tokens\n",
      "   [Node 47] test_decom_checkpoint.py: 735 tokens\n",
      "   [Node 48] test_decom_checkpoint.py: 139 tokens\n",
      "   [Node 49] __init__.py: 117 tokens\n",
      "   [Node 50] example_loss.py: 201 tokens\n",
      "   [Node 51] train_diff_retinex_plus_lolv2_real.yml: 227 tokens\n",
      "   [Node 52] train_diff_retinex_plus_lolv2_real.yml: 426 tokens\n",
      "   [Node 53] train_diff_retinex_plus_lolv2_real.yml: 279 tokens\n",
      "   [Node 54] train_diff_retinex_plus_A_LLIE.yml: 216 tokens\n",
      "   [Node 55] train_diff_retinex_plus_A_LLIE.yml: 417 tokens\n",
      "   [Node 56] train_diff_retinex_plus_A_LLIE.yml: 279 tokens\n",
      "   [Node 57] train_diff_retinex_plus_lolv2_syn.yml: 223 tokens\n",
      "   [Node 58] train_diff_retinex_plus_lolv2_syn.yml: 422 tokens\n",
      "   [Node 59] train_diff_retinex_plus_lolv2_syn.yml: 279 tokens\n",
      "   [Node 61] test_diff_retinex_plus_lolv2_real.yml: 292 tokens\n",
      "   [Node 62] train_diff_retinex_plus_lolv1.yml: 219 tokens\n",
      "   [Node 63] train_diff_retinex_plus_lolv1.yml: 419 tokens\n",
      "   [Node 64] train_diff_retinex_plus_lolv1.yml: 279 tokens\n",
      "   [Node 65] test_diff_retinex_plus_lsrw_huawei.yml: 438 tokens\n",
      "   [Node 66] test_diff_retinex_plus_lsrw_huawei.yml: 292 tokens\n",
      "   [Node 67] test_diff_retinex_plus_UHD.yml: 433 tokens\n",
      "   [Node 68] test_diff_retinex_plus_UHD.yml: 292 tokens\n",
      "   [Node 69] test_diff_retinex_plus_lolv2_syn.yml: 441 tokens\n",
      "   [Node 70] test_diff_retinex_plus_lolv2_syn.yml: 292 tokens\n",
      "   [Node 71] test_diff_retinex_plus_A-LLIE.yml: 436 tokens\n",
      "   [Node 72] test_diff_retinex_plus_A-LLIE.yml: 292 tokens\n",
      "   [Node 73] test_diff_retinex_plus_lolv1.yml: 437 tokens\n",
      "   [Node 74] test_diff_retinex_plus_lolv1.yml: 292 tokens\n",
      "   [Node 75] train_diff_retinex_plus_lsrw.yml: 226 tokens\n",
      "   [Node 76] train_diff_retinex_plus_lsrw.yml: 450 tokens\n",
      "   [Node 77] train_diff_retinex_plus_lsrw.yml: 278 tokens\n",
      "   [Node 78] test_diff-retinex_LLRW.yml: 427 tokens\n",
      "   [Node 79] test_diff-retinex_LLRW.yml: 292 tokens\n",
      "   [Node 81] test_diff_retinex_plus_lsrw_nikon.yml: 292 tokens\n",
      "   [Node 82] DRNet_model.py: 145 tokens\n",
      "   [Node 83] DRNet_model.py: 6 tokens\n",
      "   [Node 84] DRNet_model.py: 7 tokens\n",
      "   [Node 85] DRNet_model.py: 8 tokens\n",
      "   [Node 86] DRNet_model.py: 679 tokens\n",
      "   [Node 87] DRNet_model.py: 125 tokens\n",
      "   [Node 88] DRNet_model.py: 701 tokens\n",
      "   [Node 89] DRNet_model.py: 798 tokens\n",
      "   [Node 90] DRNet_model.py: 391 tokens\n",
      "   [Node 91] DRNet_model.py: 22 tokens\n",
      "   [Node 92] DRNet_model.py: 668 tokens\n",
      "   [Node 93] DRNet_model.py: 107 tokens\n",
      "   [Node 94] DRNet_model.py: 397 tokens\n",
      "   [Node 95] __init__.py: 117 tokens\n",
      "   [Node 96] options.py: 511 tokens\n",
      "   [Node 97] options.py: 10 tokens\n",
      "   [Node 98] options.py: 698 tokens\n",
      "   [Node 99] options.py: 236 tokens\n",
      "   [Node 101] prepare_example_data.py: 244 tokens\n",
      "   [Node 102] utils.py: 756 tokens\n",
      "   [Node 103] utils.py: 591 tokens\n",
      "   [Node 104] DRNet_Retinex_arch.py: 588 tokens\n",
      "   [Node 105] DRNet_Retinex_arch.py: 465 tokens\n",
      "   [Node 106] DRNet_Retinex_arch.py: 726 tokens\n",
      "   [Node 107] DRNet_Retinex_arch.py: 818 tokens\n",
      "   [Node 108] DRNet_Retinex_arch.py: 721 tokens\n",
      "   [Node 109] DRNet_Retinex_arch.py: 269 tokens\n",
      "   [Node 110] DRNet_Retinex_arch.py: 6 tokens\n",
      "   [Node 111] DRNet_Retinex_arch.py: 10 tokens\n",
      "   [Node 112] DRNet_Retinex_arch.py: 89 tokens\n",
      "   [Node 113] DRNet_Retinex_arch.py: 829 tokens\n",
      "   [Node 114] DRNet_Retinex_arch.py: 894 tokens\n",
      "   [Node 115] DRNet_Retinex_arch.py: 96 tokens\n",
      "   [Node 116] DRNet_Retinex_arch.py: 12 tokens\n",
      "   [Node 117] DRNet_Retinex_arch.py: 867 tokens\n",
      "   [Node 118] DRNet_Retinex_arch.py: 112 tokens\n",
      "   [Node 119] DRNet_Retinex_arch.py: 224 tokens\n",
      "   [Node 121] decom_arch.py: 299 tokens\n",
      "   [Node 122] decom_arch.py: 908 tokens\n",
      "   [Node 123] decom_arch.py: 262 tokens\n",
      "   [Node 124] decom_arch.py: 6 tokens\n",
      "   [Node 125] decom_arch.py: 767 tokens\n",
      "   [Node 126] decom_arch.py: 277 tokens\n",
      "   [Node 127] MoE_arch.py: 613 tokens\n",
      "   [Node 128] MoE_arch.py: 7 tokens\n",
      "   [Node 129] MoE_arch.py: 561 tokens\n",
      "   [Node 130] MoE_arch.py: 463 tokens\n",
      "   [Node 131] MoE_arch.py: 177 tokens\n",
      "   [Node 132] MoE_arch.py: 8 tokens\n",
      "   [Node 133] MoE_arch.py: 607 tokens\n",
      "   [Node 134] MoE_arch.py: 484 tokens\n",
      "   [Node 135] __init__.py: 119 tokens\n",
      "   [Node 136] sr3unet_arch.py: 591 tokens\n",
      "   [Node 137] sr3unet_arch.py: 756 tokens\n",
      "   [Node 138] sr3unet_arch.py: 751 tokens\n",
      "   [Node 139] sr3unet_arch.py: 592 tokens\n",
      "   [Node 141] sr3unet_arch.py: 8 tokens\n",
      "   [Node 142] sr3unet_arch.py: 130 tokens\n",
      "   [Node 143] sr3unet_arch.py: 549 tokens\n",
      "   [Node 144] sr3unet_arch.py: 195 tokens\n",
      "   [Node 145] sr3unet_arch.py: 304 tokens\n",
      "   [Node 146] ddpm_arch.py: 611 tokens\n",
      "   [Node 147] ddpm_arch.py: 6 tokens\n",
      "   [Node 148] ddpm_arch.py: 7 tokens\n",
      "   [Node 149] ddpm_arch.py: 179 tokens\n",
      "   [Node 150] ddpm_arch.py: 765 tokens\n",
      "   [Node 151] ddpm_arch.py: 30 tokens\n",
      "   [Node 152] ddpm_arch.py: 5 tokens\n",
      "   [Node 153] ddpm_arch.py: 75 tokens\n",
      "   [Node 154] ddpm_arch.py: 333 tokens\n",
      "   [Node 155] ddpm_arch.py: 28 tokens\n",
      "   [Node 156] ddpm_arch.py: 645 tokens\n",
      "   [Node 157] ddpm_arch.py: 121 tokens\n",
      "   [Node 158] ddpm_arch.py: 32 tokens\n",
      "   [Node 159] ddpm_arch.py: 731 tokens\n",
      "   [Node 161] __init__.py: 116 tokens\n",
      "   [Node 162] lol_dataset.py: 110 tokens\n",
      "   [Node 163] lol_dataset.py: 7 tokens\n",
      "   [Node 164] lol_dataset.py: 7 tokens\n",
      "   [Node 165] lol_dataset.py: 250 tokens\n",
      "   [Node 166] lol_dataset.py: 8 tokens\n",
      "   [Node 167] lol_dataset.py: 675 tokens\n",
      "   [Node 168] lol_dataset.py: 355 tokens\n",
      "   [Node 169] lol_dataset.py: 13 tokens\n",
      "\n",
      "--- Token Stats ---\n",
      "‚úÖ Total Tokens: 58,887\n",
      "   Average:      346.4\n",
      "   Max:          912\n",
      "   Min:          4\n",
      "   Est. Cost:    $0.008833\n"
     ]
    }
   ],
   "source": [
    "calculate_token_counts(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa5c2b",
   "metadata": {},
   "source": [
    "%pip install chromadb llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e85be8",
   "metadata": {},
   "source": [
    "# Step 9: Vector Store Setup (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c741c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e97e0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Opening database at ./github_rag_engine/data/chroma_db...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize ChromaDB (Persistent)\n",
    "db_path = \"./github_rag_engine/data/chroma_db\"\n",
    "print(f\"üìÇ Opening database at {db_path}...\")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "# 2. Create Collection\n",
    "# A collection is like a table. We name it specifically for this repo.\n",
    "collection = chroma_client.get_or_create_collection(\"github_rag_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68cc3680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving index to disk... (This might take a moment to write to SQLite)\n",
      "Index saved! Collection now has 170 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 3. Connect LlamaIndex to Chroma\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# 4. Build the Index (Persist to Disk)\n",
    "# We pass 'nodes_with_embeddings' so it doesn't re-calculate costs!\n",
    "print(\"üíæ Saving index to disk... (This might take a moment to write to SQLite)\")\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes_with_embeddings,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "print(f\"Index saved! Collection now has {collection.count()} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19154420",
   "metadata": {},
   "source": [
    "# Step 10: Retrieval & Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c03c87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5b301115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The Retriever\n",
    "# We fetch slightly more (k=10) than we need, so we can filter them down later.\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "# 2. Post-Processor: \"Quality Control\"\n",
    "# This acts as a gatekeeper. Nodes below the cutoff are dropped.\n",
    "# Note: For OpenAI text-embedding-3, scores are often lower, so we start with 0.4.\n",
    "similarity_cutoff = SimilarityPostprocessor(similarity_cutoff=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f90f5d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query Engine Ready (Top-10 Retrieval -> Cutoff 0.25)\n"
     ]
    }
   ],
   "source": [
    "# 3. The Query Engine\n",
    "# Retriever (Fetch 10) -> PostProcessor (Filter bad ones) -> LLM (Synthesize Answer)\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[similarity_cutoff]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Query Engine Ready (Top-10 Retrieval -> Cutoff 0.25)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a423d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Inspecting retrieval for: 'Where is the main training code implemented?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 15:30:48,880 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] ‚úÖ KEEP (Score: 0.3118) File: train_pipline.py\n",
      "[1] ‚úÖ KEEP (Score: 0.3034) File: train_pipline.py\n",
      "[2] ‚úÖ KEEP (Score: 0.2961) File: train_pipline.py\n",
      "[3] ‚úÖ KEEP (Score: 0.2897) File: train_pipline.py\n",
      "[4] ‚úÖ KEEP (Score: 0.2717) File: train_decom.py\n",
      "[5] ‚úÖ KEEP (Score: 0.2675) File: train_decom.py\n",
      "[6] ‚úÖ KEEP (Score: 0.2633) File: train.py\n",
      "[7] ‚úÖ KEEP (Score: 0.2630) File: train_diff_retinex_plus_lolv2_real.yml\n",
      "[8] ‚úÖ KEEP (Score: 0.2621) File: train_decom.py\n",
      "[9] ‚úÖ KEEP (Score: 0.2610) File: README.md\n"
     ]
    }
   ],
   "source": [
    "test_query = \"Where is the main training code implemented?\"\n",
    "print(f\"\\nüîé Inspecting retrieval for: '{test_query}'\")\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(test_query)\n",
    "\n",
    "for i, node in enumerate(retrieved_nodes):\n",
    "    # Check if it would pass our cutoff\n",
    "    status = \"‚úÖ KEEP\" if node.score >= 0.25 else \"‚ùå DROP\"\n",
    "    print(f\"[{i}] {status} (Score: {node.score:.4f}) File: {node.metadata.get('file_name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fac84a",
   "metadata": {},
   "source": [
    "# Step 11: Custom Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9610d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f37a8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_str = (\n",
    "    \"You are a Senior Software Engineer acting as a code assistant.\\n\"\n",
    "    \"You have been provided with context from a GitHub repository.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given this context, answer the following technical question.\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"1. Use ONLY the context provided. If the answer is not in the code, say 'I cannot find the answer in the provided files'.\\n\"\n",
    "    \"2. When citing code, mention the specific filename (e.g., 'in train.py...').\\n\"\n",
    "    \"3. Be concise but technical.\\n\"\n",
    "    \"\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "49e513b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom 'Senior Engineer' prompt applied.\n"
     ]
    }
   ],
   "source": [
    "text_qa_template = PromptTemplate(qa_prompt_str)\n",
    "# 3. Update the Query Engine\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": text_qa_template}\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Custom 'Senior Engineer' prompt applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f109ed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Querying: 'What is the main model, in which file is it present and what is the overall goal of the model?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 15:30:51,797 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:30:54,223 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ü§ñ Expert Response ---\n",
      "The main model is `DRNetModel`, which is present in the file `github_rag_engine/data/repos/Diff-Retinex-Plus/models/DRNet_model.py`. The overall goal of the model is to perform image restoration, specifically enhancing images by utilizing a combination of networks including a denoising function and a decomposition network to improve the quality of low-quality images (lq) and generate high-quality outputs (gt). The model incorporates various loss functions to optimize its performance during training.\n"
     ]
    }
   ],
   "source": [
    "# --- TEST WITH CUSTOM PROMPT ---\n",
    "query_str = \"What is the main model, in which file is it present and what is the overall goal of the model?\"\n",
    "print(f\"\\n‚ùì Querying: '{query_str}'\")\n",
    "\n",
    "response = query_engine.query(query_str)\n",
    "print(\"\\n--- ü§ñ Expert Response ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccd4b6",
   "metadata": {},
   "source": [
    "\n",
    "# Step 12: Automated Evaluation (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79b045f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce40a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=Settings.llm)\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=Settings.llm)\n",
    "\n",
    "def evaluate_query(query_str):\n",
    "    print(f\"üß™ Evaluating Query: '{query_str}'\")\n",
    "    response = query_engine.query(query_str)\n",
    "    pass_faithfulness = faithfulness_evaluator.evaluate_response(response=response)\n",
    "    pass_relevancy = relevancy_evaluator.evaluate_response(query=query_str, response=response)\n",
    "    \n",
    "    # 3. Report Results\n",
    "    print(f\"   Shape of Response: {len(str(response))} chars\")\n",
    "    print(f\"   Sources Retrieved: {len(response.source_nodes)}\")\n",
    "    \n",
    "    print(\"\\n--- üë©‚Äç‚öñÔ∏è Verdict ---\")\n",
    "    print(f\"   Faithfulness (No Hallucinations): {'‚úÖ PASS' if pass_faithfulness.passing else '‚ùå FAIL'}\")\n",
    "    print(f\"   Relevancy    (Answered Question): {'‚úÖ PASS' if pass_relevancy.passing else '‚ùå FAIL'}\")\n",
    "    \n",
    "    if not pass_faithfulness.passing:\n",
    "        print(f\"   ‚ö†Ô∏è Reason: {pass_faithfulness.feedback}\")\n",
    "    if not pass_relevancy.passing:\n",
    "        print(f\"   ‚ö†Ô∏è Reason: {pass_relevancy.feedback}\")\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4628cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Evaluating Query: 'How are the losses calculated in this repo? detailed check'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 15:32:33,918 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:32:55,209 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:32:59,718 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:32:59,795 - INFO - Retrying request to /chat/completions in 0.476411 seconds\n",
      "2026-02-04 15:33:01,596 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Shape of Response: 1601 chars\n",
      "   Sources Retrieved: 10\n",
      "\n",
      "--- üë©‚Äç‚öñÔ∏è Verdict ---\n",
      "   Faithfulness (No Hallucinations): ‚úÖ PASS\n",
      "   Relevancy    (Answered Question): ‚úÖ PASS\n",
      "\n",
      "--- Actual Answer Content ---\n",
      "The losses in this repository are calculated primarily in the `Decom_Loss` class found in `loss_decom_TDN.py`. The loss computation involves several components:\n",
      "\n",
      "1. **Reconstruction Losses**:\n",
      "   - `self.recon_loss_low`: Calculated using L1 loss between the product of `R_low` and `L_low_3` and `I_low`.\n",
      "   - `self.recon_loss_high`: Calculated using L1 loss between the product of `R_high` and `L_high_3` and `I_high`.\n",
      "   - `self.recon_loss_crs_low`: Calculated using L1 loss between the product of `R_high` and `L_low_3` and `I_low`.\n",
      "   - `self.recon_loss_crs_high`: Calculated using L1 loss between the product of `R_low` and `L_high_3` and `I_high`.\n",
      "\n",
      "2. **Equalization Loss**:\n",
      "   - `self.equal_R_loss`: Calculated using L1 loss between `R_low` and the detached `R_high`.\n",
      "\n",
      "3. **Smoothness Losses**:\n",
      "   - `self.Ismooth_loss_low`: Calculated using the `smooth` method, which computes the gradient of `L_low` with respect to `R_low`.\n",
      "   - `self.Ismooth_loss_high`: Similar to `Ismooth_loss_low`, but for `L_high` and `R_high`.\n",
      "\n",
      "4. **Total Loss**:\n",
      "   - The total decomposition loss `self.loss_Decom` is computed as a weighted sum of the individual losses:\n",
      "     ```python\n",
      "     self.loss_Decom = self.recon_loss_high + 0.3 * self.recon_loss_low + 0.001 * self.recon_loss_crs_low + \\\n",
      "                       0.001 * self.recon_loss_crs_high + 0.1 * (self.Ismooth_loss_low + self.Ismooth_loss_high) + 0.1 * self.equal_R_loss\n",
      "     ```\n",
      "\n",
      "This structure allows for a comprehensive evaluation of the model's performance in reconstructing images while maintaining smoothness and consistency across different scales.\n"
     ]
    }
   ],
   "source": [
    "eval_query_str = \"How are the losses calculated in this repo? detailed check\"\n",
    "response = evaluate_query(eval_query_str)\n",
    "\n",
    "print(\"\\n--- Actual Answer Content ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99ec78b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Evaluating Query: 'Do we need any predefined .pth file to run this model, We yes what to do for new dataset'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 15:35:14,941 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:35:20,005 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:35:20,078 - INFO - Retrying request to /chat/completions in 0.405129 seconds\n",
      "2026-02-04 15:35:21,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2026-02-04 15:35:21,397 - INFO - Retrying request to /chat/completions in 0.436064 seconds\n",
      "2026-02-04 15:35:22,431 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Shape of Response: 374 chars\n",
      "   Sources Retrieved: 10\n",
      "\n",
      "--- üë©‚Äç‚öñÔ∏è Verdict ---\n",
      "   Faithfulness (No Hallucinations): ‚úÖ PASS\n",
      "   Relevancy    (Answered Question): ‚úÖ PASS\n",
      "\n",
      "--- Actual Answer Content ---\n",
      "Yes, you need predefined .pth files (pretrained weights) to run the model. For a new dataset, you should download the pretrained weights and place them in the `pretrained_weights` folder as mentioned in the README.md file. The specific links for downloading the pretrained weights for various datasets are provided in the README.md under the section \"3. Pretrained Weights\".\n"
     ]
    }
   ],
   "source": [
    "eval_query_str = \"Do we need any predefined .pth file to run this model, We yes what to do for new dataset\"\n",
    "response = evaluate_query(eval_query_str)\n",
    "\n",
    "print(\"\\n--- Actual Answer Content ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2595b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

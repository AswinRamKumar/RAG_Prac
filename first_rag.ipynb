{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Environment & Setup\n",
    "We are building a production RAG system. We start in a notebook to **prototype components** before moving them to Python modules.\n",
    "\n",
    "### Step 1: Load Environment Variables\n",
    "Since our `.env` is in a sibling directory, we need to load it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded .env file\n",
      "âœ… API Key found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define path to the .env file in the sibling directory\n",
    "# Adjust 'RAG systems using LlamaIndex' if your folder name is different\n",
    "env_path = Path(\"..\") / \"RAG systems using LlamaIndex\" / \".env\"\n",
    "\n",
    "# 2. Load it\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    print(\"âœ… Loaded .env file\")\n",
    "else:\n",
    "    print(f\"âŒ Could not find .env at: {env_path.resolve()}\")\n",
    "\n",
    "# 3. Verify\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"âœ… API Key found\")\n",
    "else:\n",
    "    print(\"âš ï¸ API Key not found. Check your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: The Ingestion Logic\n",
    "**Design Decision**: Clone **Everything**, Filter **Later**.\n",
    "*   **Cloning**: Use `git clone`. It's faster and simpler to just pull the whole repo than to try and \"sparse checkout\" file-by-file.\n",
    "*   **Filtering**: We will apply a **Blacklist** (exclude `.png`, `.pth`, `.git`) during the document loading phase. This keeps the local disk faithful to reality but protects the embedding model from garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ Repo already exists at: github_rag_engine/data/repos/Diff-Retinex-Plus\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# CONFIG\n",
    "REPO_URL = \"https://github.com/XunpengYi/Diff-Retinex-Plus\"\n",
    "STORAGE_PATH = Path(\"./github_rag_engine/data/repos\")\n",
    "REPO_NAME = REPO_URL.split(\"/\")[-1]\n",
    "LOCAL_REPO_PATH = STORAGE_PATH / REPO_NAME\n",
    "\n",
    "def clone_repo(url, path):\n",
    "    if path.exists():\n",
    "        print(f\"â„¹ï¸ Repo already exists at: {path}\")\n",
    "        # In production, we would git pull here\n",
    "        return\n",
    "    \n",
    "    print(f\"â³ Cloning {url}...\")\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", url, str(path)], check=True)\n",
    "        print(\"âœ… Clone successful\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Clone failed: {e}\")\n",
    "\n",
    "# 1. Run the clone\n",
    "clone_repo(REPO_URL, LOCAL_REPO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Filtering & Processing\n",
    "Now we iterate through the files. \n",
    "\n",
    "**The Rule**: skip if extension is in `IGNORED_EXTENSIONS` OR if it is a hidden file/directory (starts with `.`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORED_EXTENSIONS = {\n",
    "    # Images\n",
    "    \".png\", \".jpg\", \".jpeg\", \".gif\", \".svg\", \".ico\",\n",
    "    # Compiled / Binary\n",
    "    \".pyc\", \".o\", \".exe\", \".dll\", \".so\", \".dylib\",\n",
    "    # Models / Data\n",
    "    \".pth\", \".pt\", \".onnx\", \".pkl\", \".bin\", \".data\",\n",
    "    # Git\n",
    "    \".git\",\n",
    "    # Lock files (Optional - debating this)\n",
    "    \".lock\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_source_files(directory):\n",
    "    source_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # 1. Remove hidden dirs like .git in-place to avoid traversing them\n",
    "        dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n",
    "        \n",
    "        for file in files:\n",
    "            # 2. Skip hidden files\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "                \n",
    "            # 3. Check extension blacklist\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            if ext in IGNORED_EXTENSIONS:\n",
    "                continue\n",
    "            \n",
    "            full_path = Path(root) / file\n",
    "            source_files.append(full_path)\n",
    "            \n",
    "    return source_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 valid source files\n",
      "Sample: ['test.py', 'README.md', 'test_UHD.py', 'train.py', 'environment.yaml']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the filter\n",
    "files = get_source_files(LOCAL_REPO_PATH)\n",
    "print(f\"Found {len(files)} valid source files\")\n",
    "print(\"Sample:\", [f.name for f in files[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The \"Repo Map\" (The \"What was here\" context)\n",
    "You made a great point: if we skip `.pth` files, the LLM won't know they exist. \n",
    "\n",
    "**The Solution**: We create a single special document called the **Repo Map**. \n",
    "This is a text file containing the tree structure of the *entire* repo (including the files we ignored). We index this first. This gives the application \"Spatial Awareness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repo_map(root_dir):\n",
    "    lines = []\n",
    "    root_path = Path(root_dir)\n",
    "\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if \".git\" in dirs:\n",
    "            dirs.remove(\".git\")\n",
    "\n",
    "        for f in files:\n",
    "            rel_path = Path(root, f).relative_to(root_path)\n",
    "            ext = rel_path.suffix.lower()\n",
    "\n",
    "            marker = \"\"\n",
    "            if f.startswith(\".\") or ext in IGNORED_EXTENSIONS:\n",
    "                marker = \" [IGN]\"\n",
    "\n",
    "            lines.append(f\"{rel_path.as_posix()}{marker}\")\n",
    "\n",
    "    return \"\\n\".join(sorted(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- REPO MAP PREVIEW ---\n",
      "README.md\n",
      "archs/DRNet_Retinex_arch.py\n",
      "archs/MoE_arch.py\n",
      "archs/__init__.py\n",
      "archs/ddpm_arch.py\n",
      "archs/decom_arch.py\n",
      "archs/sr3unet_arch.py\n",
      "asserts/framework.png [IGN]\n",
      "data/__init__.py\n",
      "data/lol_dataset.py\n",
      "dataset/put_the_dataset_here.md\n",
      "decom_tools/decom_arch.py\n",
      "decom_tools/loss_decom_TDN.py\n",
      "decom_tools/my_dataset.py\n",
      "decom_tools/test_decom_checkpoint.py\n",
      "decom_tools/train_decom.py\n",
      "decom_tools/transforms.py\n",
      "decom_tools/utils.py\n",
      "decom_weights/put_the_decom_weights_here.md\n",
      "environment.yaml\n",
      "lolv1_offical_results/1.png [IGN]\n",
      "lolv1_offical_results/111.png [IGN]\n",
      "lolv1_offical_results/146.png [IGN]\n",
      "lolv1_offical_results/179.png [IGN]\n",
      "lolv1_offical_results/22.png [IGN]\n",
      "lolv1_offical_results/23.png [IGN]\n",
      "lolv1_offical_results/493.png [IGN]\n",
      "lolv1_offical_results/547.png [IGN]\n",
      "lolv1_offical_results/55.png [IGN]\n",
      "lolv1_offical_results/665.png [IGN]\n",
      "lolv1_offical_results/669.png [IGN]\n",
      "lolv1_offical_results/748.png [IGN]\n",
      "lolv1_offical_results/778.png [IGN]\n",
      "lolv1_offical_results/780.png [IGN]\n",
      "lolv1_offical_results/79.png [IGN]\n",
      "losses/__init__.py\n",
      "losses/example_loss.py\n",
      "models/DRNet_model.py\n",
      "models/__init__.py\n",
      "options/test_diff-retinex_LLRW.yml\n",
      "options/test_diff_retinex_plus_A-LLIE.yml\n",
      "options/test_diff_retinex_plus_UHD.yml\n",
      "options/test_diff_retinex_plus_lolv1.yml\n",
      "options/test_diff_retinex_plus_lolv2_real.yml\n",
      "options/test_diff_retinex_plus_lolv2_syn.yml\n",
      "options/test_diff_retinex_plus_lsrw_huawei.yml\n",
      "options/test_diff_retinex_plus_lsrw_nikon.yml\n",
      "options/train_diff_retinex_plus_A_LLIE.yml\n",
      "options/train_diff_retinex_plus_lolv1.yml\n",
      "options/train_diff_retinex_plus_lolv2_real.yml\n",
      "options/train_diff_retinex_plus_lolv2_syn.yml\n",
      "options/train_diff_retinex_plus_lsrw.yml\n",
      "pretrained_weights/put_the_pretrained_weights_here.md\n",
      "scripts/options.py\n",
      "scripts/prepare_example_data.py\n",
      "scripts/utils.py\n",
      "test.py\n",
      "test_UHD.py\n",
      "train.py\n",
      "train_pipline.py\n"
     ]
    }
   ],
   "source": [
    "# Generate and Print\n",
    "repo_map_str = generate_repo_map(LOCAL_REPO_PATH)\n",
    "print(\"--- REPO MAP PREVIEW ---\")\n",
    "print(repo_map_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Loading with Metadata\n",
    "Now we load the *content* of the valid files. \n",
    "\n",
    "**Crucial**: We inject `file_metadata`. Every chunk must know its own filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Repo Map Document\n",
    "map_doc = Document(\n",
    "    text=repo_map_str,\n",
    "    metadata={\"file_path\": \"REPO_STRUCTURE.txt\", \"type\": \"structure\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 2. Helper to extract metadata from files\n",
    "def file_metadata_extractor(file_path):\n",
    "    # Get modification time\n",
    "    #stat = os.stat(file_path)\n",
    "    #timestamp = stat.st_mtime\n",
    "    # Format as readable string\n",
    "    #last_mod = datetime.datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return {\n",
    "        \"file_path\": file_path,\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"extension\": os.path.splitext(file_path)[1],\n",
    "        \"repo_path\": os.path.dirname(file_path),\n",
    "        #\"last_modified\": last_mod\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 44 code documents + 1 Repo Map\n"
     ]
    }
   ],
   "source": [
    "if not files:\n",
    "    print(\"âš ï¸ No files found to index!\")\n",
    "    documents = []\n",
    "else:\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_files=[str(f) for f in files],\n",
    "        file_metadata=file_metadata_extractor\n",
    "    )\n",
    "    code_documents = reader.load_data()\n",
    "    all_documents = [map_doc] + code_documents\n",
    "    print(f\"âœ… Loaded {len(code_documents)} code documents + 1 Repo Map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus/test.py', 'file_name': 'test.py', 'extension': '.py', 'repo_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus'}\n"
     ]
    }
   ],
   "source": [
    "print(code_documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Settings (OpenAI)\n",
    "We use **OpenAI** for both embedding and LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_llm_settings():\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "    Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "    print(\"âœ… Configured: text-embedding-3-small & GPT-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured: text-embedding-3-small & GPT-4o-mini\n"
     ]
    }
   ],
   "source": [
    "configure_llm_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Adaptive Chunking (File-Type Aware & Tiktoken)\n",
    "You are absolutely right. `.py` is handled by `CodeSplitter`, but `.md` needs `MarkdownNodeParser`, and `.yaml` needs standard text splitting.\n",
    "\n",
    "**Refinement**: We use **tiktoken** to ensure our chunks are measured in *tokens*, not characters. This is critical for maximizing the LLM's context window usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.node_parser import CodeSplitter, MarkdownNodeParser, SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082b91a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_adaptive(docs):\n",
    "    alln = []\n",
    "    tok = tiktoken.encoding_for_model(\"gpt-4o-mini\").encode\n",
    "\n",
    "    ps = CodeSplitter(\n",
    "        language=\"python\",\n",
    "        chunk_lines=150,\n",
    "        chunk_lines_overlap=30,\n",
    "        max_chars=3000\n",
    "    )\n",
    "\n",
    "    ms = MarkdownNodeParser(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    ts = SentenceSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        tokenizer=tok\n",
    "    )\n",
    "\n",
    "    for doc in docs:\n",
    "        fn = doc.metadata.get(\"file_name\", \"\")\n",
    "        ext = doc.metadata.get(\"extension\", \"\").lower()\n",
    "\n",
    "        if ext == \".py\":\n",
    "            nds = ps.get_nodes_from_documents([doc])\n",
    "        elif ext == \".md\":\n",
    "            nds = ms.get_nodes_from_documents([doc])\n",
    "        else:\n",
    "            nds = ts.get_nodes_from_documents([doc])\n",
    "\n",
    "        for i, n in enumerate(nds):\n",
    "            n.metadata[\"chunk_id\"] = i\n",
    "            n.metadata[\"file_ext\"] = ext\n",
    "            n.metadata[\"file_name\"] = fn\n",
    "\n",
    "            pre = nds[i - 1].text if i > 0 else \"\"\n",
    "            post = nds[i + 1].text if i < len(nds) - 1 else \"\"\n",
    "\n",
    "            n.metadata[\"pre_context\"] = pre\n",
    "            n.metadata[\"post_context\"] = post\n",
    "\n",
    "        alln.extend(nds)\n",
    "\n",
    "    return alln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e941c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_nodes(nodes, sample_exts=(\".py\", \".md\", \".yaml\", \".yml\")):\n",
    "    seen = set()\n",
    "    for n in nodes:\n",
    "        ext = n.metadata.get(\"file_ext\", \"\")\n",
    "        if ext in sample_exts and ext not in seen:\n",
    "            seen.add(ext)\n",
    "            print(\"\\n--- Sample Chunk for\", ext, n.metadata.get(\"file_name\", \"\"))\n",
    "            print(\"[Node Type: {}]\".format(n.class_name()))\n",
    "            print(\"chunk_id:\", n.metadata.get(\"chunk_id\"))\n",
    "            txt = n.text[:400].replace(\"\\n\", \"\\\\n\")\n",
    "            print(txt + \"...\")\n",
    "            pre = n.metadata.get(\"pre_context\", \"\")\n",
    "            post = n.metadata.get(\"post_context\", \"\")\n",
    "            print(\"\\npre_context (first 200 chars):\")\n",
    "            print(pre[:200].replace(\"\\n\", \"\\\\n\"))\n",
    "            print(\"\\npost_context (first 200 chars):\")\n",
    "            print(post[:200].replace(\"\\n\", \"\\\\n\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "216947a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated nodes: 170\n",
      "\n",
      "--- Sample Chunk for .py test.py\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "import logging\\nimport torch\\nfrom os import path as osp\\nimport archs\\nimport data\\nimport losses\\nimport models\\n\\nfrom basicsr.data import build_dataloader, build_dataset\\nfrom basicsr.models import build_model\\nfrom basicsr.utils import get_env_info, get_root_logger, get_time_str, make_exp_dirs\\nfrom basicsr.utils.options import dict2str, parse_options\\n\\n\\ndef test_pipeline(root_path):\\n    # parse options,...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "\n",
      "\n",
      "--- Sample Chunk for .md README.md\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "# [TPAMI 2025] Diff-Retinex++: Retinex-Driven Reinforced Diffusion Model for Low-Light Image Enhancement...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "### [Paper](https://ieeexplore.ieee.org/abstract/document/10974676) | [Code](https://github.com/XunpengYi/Diff-Retinex-Plus) \\n\\n**Diff-Retinex++: Retinex-Driven Reinforced Diffusion Model for Low-Light\n",
      "\n",
      "--- Sample Chunk for .yaml environment.yaml\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "name: diff_retinex_plus_env\\nchannels:\\n  - pytorch\\n  - conda-forge\\n  - defaults\\ndependencies:\\n  - _libgcc_mutex=0.1=conda_forge\\n  - _openmp_mutex=4.5=2_gnu\\n  - aom=3.5.0=h27087fc_0\\n  - blas=1.0=mkl\\n  - bzip2=1.0.8=h4bc722e_7\\n  - ca-certificates=2024.8.30=hbcca054_0\\n  - cairo=1.16.0=h35add3b_1015\\n  - cudatoolkit=11.3.1=hb98b00a_13\\n  - dav1d=1.2.1=hd590300_0\\n  - ffmpeg=6.0.0=gpl_hdbbbd96_103\\n  - font...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "7.9=hb077bed_0\\n  - graphite2=1.3.13=h59595ed_1003\\n  - harfbuzz=7.3.0=hdb3a94d_0\\n  - icu=72.1=hcb278e6_0\\n  - intel-openmp=2022.0.1=h06a4308_3633\\n  - jpeg=9e=h0b41bf4_3\\n  - lame=3.100=h166bdaf_1003\\n  - \n",
      "\n",
      "--- Sample Chunk for .yml train_diff_retinex_plus_lolv2_real.yml\n",
      "[Node Type: TextNode]\n",
      "chunk_id: 0\n",
      "# general settings\\nname: diff-retinex_plus_lolv2_real_train\\nmodel_type: DRNetModel\\nnum_gpu: 2  # set num_gpu: 0 for cpu mode\\nmanual_seed: 1234\\n\\n# dataset and data loader settings\\ndatasets:\\n  train:\\n    name: train\\n    type: LoLDataset\\n    gt_root: \"./dataset/LOL-v2/Real_captured/Train/Normal\"\\n    input_root: \"./dataset/LOL-v2/Real_captured/Train/Low\"\\n    input_mode: crop\\n    crop_size: 160\\n\\n    co...\n",
      "\n",
      "pre_context (first 200 chars):\n",
      "\n",
      "\n",
      "post_context (first 200 chars):\n",
      "# data loader\\n    use_shuffle: true\\n    num_worker_per_gpu: 16\\n    batch_size_per_gpu: 6\\n    dataset_enlarge_ratio: 1\\n    prefetch_mode: ~\\n\\n  val:\\n    name: LOL\\n    type: LoLDataset\\n    gt_root: \"./da\n",
      "nodes with empty file_ext: 0\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "nodes = get_nodes_adaptive(code_documents)\n",
    "print(\"generated nodes:\", len(nodes))\n",
    "inspect_nodes(nodes)\n",
    "empty_exts = [n for n in nodes if not n.metadata.get(\"file_ext\")]\n",
    "print(\"nodes with empty file_ext:\", len(empty_exts))\n",
    "if empty_exts:\n",
    "    for n in empty_exts[:5]:\n",
    "        print(\"file_name:\", n.metadata.get(\"file_name\"), \"chunk_id:\", n.metadata.get(\"chunk_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8abf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus/scripts/options.py',\n",
       " 'file_name': 'options.py',\n",
       " 'extension': '.py',\n",
       " 'repo_path': 'github_rag_engine/data/repos/Diff-Retinex-Plus/scripts',\n",
       " 'chunk_id': 3,\n",
       " 'file_ext': '.py',\n",
       " 'pre_context': 'parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'-opt\\', type=str, default=\"options/train_diff_retinex_plus_lolv1.yml\", help=\\'Path to option YAML file.\\')\\n    parser.add_argument(\\'--launcher\\', choices=[\\'none\\', \\'pytorch\\', \\'slurm\\'], default=\\'none\\', help=\\'job launcher\\')\\n    parser.add_argument(\\'--auto_resume\\', action=\\'store_true\\')\\n    parser.add_argument(\\'--debug\\', action=\\'store_true\\')\\n    parser.add_argument(\\'--local_rank\\', type=int, default=0)\\n    parser.add_argument(\\n        \\'--force_yml\\', nargs=\\'+\\', default=None, help=\\'Force to update yml files. Examples: train:ema_decay=0.999\\')\\n    args = parser.parse_args()\\n\\n    # parse yml to dict\\n    with open(args.opt, mode=\\'r\\') as f:\\n        opt = yaml.load(f, Loader=ordered_yaml()[0])\\n\\n    # distributed settings\\n    if args.launcher == \\'none\\':\\n        opt[\\'dist\\'] = False\\n        print(\\'Disable distributed.\\', flush=True)\\n    else:\\n        opt[\\'dist\\'] = True\\n        if args.launcher == \\'slurm\\' and \\'dist_params\\' in opt:\\n            init_dist(args.launcher, **opt[\\'dist_params\\'])\\n        else:\\n            init_dist(args.launcher)\\n    opt[\\'rank\\'], opt[\\'world_size\\'] = get_dist_info()\\n\\n    # random seed\\n    seed = opt.get(\\'manual_seed\\')\\n    if seed is None:\\n        seed = random.randint(1, 10000)\\n        opt[\\'manual_seed\\'] = seed\\n    set_random_seed(seed + opt[\\'rank\\'])\\n\\n    # force to update yml options\\n    if args.force_yml is not None:\\n        for entry in args.force_yml:\\n            # now do not support creating new keys\\n            keys, value = entry.split(\\'=\\')\\n            keys, value = keys.strip(), value.strip()\\n            value = _postprocess_yml_value(value)\\n            eval_str = \\'opt\\'\\n            for key in keys.split(\\':\\'):\\n                eval_str += f\\'[\"{key}\"]\\'\\n            eval_str += \\'=value\\'\\n            # using exec function\\n            exec(eval_str)\\n\\n    opt[\\'auto_resume\\'] = args.auto_resume\\n    opt[\\'is_train\\'] = is_train\\n\\n    # debug setting\\n    if args.debug and not opt[\\'name\\'].startswith(\\'debug\\'):\\n        opt[\\'name\\'] = \\'debug_\\' + opt[\\'name\\']\\n\\n    if opt[\\'num_gpu\\'] == \\'auto\\':\\n        opt[\\'num_gpu\\'] = torch.cuda.device_count()\\n\\n    # datasets\\n    for phase, dataset in opt[\\'datasets\\'].items():\\n        phase = phase.split(\\'_\\')[0]\\n        dataset[\\'phase\\'] = phase\\n        if \\'scale\\' in opt:\\n            dataset[\\'scale\\'] = opt[\\'scale\\']\\n        if dataset.get(\\'dataroot_gt\\') is not None:\\n            dataset[\\'dataroot_gt\\'] = osp.expanduser(dataset[\\'dataroot_gt\\'])\\n        if dataset.get(\\'dataroot_lq\\') is not None:\\n            dataset[\\'dataroot_lq\\'] = osp.expanduser(dataset[\\'dataroot_lq\\'])\\n\\n    # paths\\n    for key, val in opt[\\'path\\'].items():\\n        if (val is not None) and (\\'resume_state\\' in key or \\'pretrain_network\\' in key):\\n            opt[\\'path\\'][key] = osp.expanduser(val)',\n",
       " 'post_context': \"@master_only\\ndef copy_opt_file(opt_file, experiments_root):\\n    import sys\\n    import time\\n    from shutil import copyfile\\n    cmd = ' '.join(sys.argv)\\n    filename = osp.join(experiments_root, osp.basename(opt_file))\\n    copyfile(opt_file, filename)\\n\\n    with open(filename, 'r+') as f:\\n        lines = f.readlines()\\n        lines.insert(0, f'# GENERATE TIME: {time.asctime()}\\\\n# CMD:\\\\n# {cmd}\\\\n\\\\n')\\n        f.seek(0)\\n        f.writelines(lines)\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[99].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3261fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Step 8: Embeddings with Caching\n",
    "# ==========================================\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.storage.kvstore import SimpleKVStore\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6778209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup the Cache Storage\n",
    "CACHE_FILE = \"./github_rag_engine/cache/ingestion_cache.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65ac3f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ Creating new ingestion cache...\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.dirname(CACHE_FILE), exist_ok=True)\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(f\"ðŸ“– Loading ingestion cache from {CACHE_FILE}...\")\n",
    "    kv_store = SimpleKVStore.from_persist_path(CACHE_FILE)\n",
    "else:\n",
    "    print(\"âœ¨ Creating new ingestion cache...\")\n",
    "    kv_store = SimpleKVStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "282593e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the Pipeline\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        Settings.embed_model  # The embedding model is configured earlier\n",
    "    ],\n",
    "    cache=IngestionCache(cache=kv_store)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d601e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Generating embeddings for 170 nodes (with caching)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-03 22:15:38,563 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-03 22:15:40,008 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings generated and cache saved to ./github_rag_engine/cache/ingestion_cache.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸš€ Generating embeddings for {len(nodes)} nodes (with caching)...\")\n",
    "nodes_with_embeddings = pipeline.run(nodes=nodes)\n",
    "kv_store.persist(CACHE_FILE)\n",
    "print(f\"âœ… Embeddings generated and cache saved to {CACHE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eeb47a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Embedding (First 5 dims): [-0.014153656549751759, -0.018069013953208923, 0.06104068085551262, -0.055009569972753525, -0.01625724881887436]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sample Embedding (First 5 dims): {nodes_with_embeddings[0].embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed7007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a205602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
